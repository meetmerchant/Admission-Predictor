# This notebook is coded in Python3
# The installation guide explains the installation of required Python modules

import numpy as np  # linear algebra
import pandas as pd # data processing, CSV file I/O

# %% [code]
# Read the dataset into a dataframe
df = pd.read_csv('../input/adm_pred.csv')
df.head()

# %% [code]
#Comprehensive description of data
print('Rows     :',df.shape[0])
print('Columns  :',df.shape[1])
print('\nFeatures :\n     :',df.columns.tolist())
print('\nMissing values    :',df.isnull().values.sum())
print('\nUnique values :  \n',df.nunique())

# %% [code]
# Visualize the diversity of applicant for a particular university
import matplotlib.pyplot as plt   # plotting library
import seaborn as sns             # attractive visualizations

sns.set(style="white", palette="muted", color_codes=True)

fig = sns.distplot(df['GRE Score'], kde=False)
plt.title("Distribution of GRE Scores")
plt.show()

fig = sns.distplot(df['TOEFL Score'], kde=False)
plt.title("Distribution of TOEFL Scores")
plt.show()

fig = sns.distplot(df['University Rating'], kde=False)
plt.title("Distribution of University Rating")
plt.show()

fig = sns.distplot(df['SOP'], kde=False)
plt.title("Distribution of SOP Ratings")
plt.show()

fig = sns.distplot(df['CGPA'], kde=False)
plt.title("Distribution of CGPA")
plt.show()

plt.show()

# %% [code]
# Gender Ratio Visualization
from sklearn import preprocessing  # label encoding of categorical data

# Calculate number of male and female applicants in the data
le = preprocessing.LabelEncoder()
xyz = le.fit_transform(df['Gender'])
f=0
m=0
for i in xyz:
    if (i==0):
        f=f+1
    else:
        m=m+1

# Plot the pie chart
fig = plt.figure()
ax = fig.add_axes([0,0,1,1])
ax.axis('equal')
labels = 'Male', 'Female'
sizes = [m,f]
colors = ['#308CCE', '#E27FD3']
ax.pie(sizes, labels=labels, colors=colors, autopct ='% 1.1f %%', shadow = True)
plt.title('Gender Ratio')
plt.show()

# %% [code]
# Nationality Diversity
df['Nation'].value_counts().plot(kind='bar', color = 'orange', title = 'Nationality Distribution')

# %% [code]
# Average scores
df.rename(columns={'Serial No.':'Srno','GRE Score':'GRE','TOEFL Score':'TOEFL','University Rating':'UnivRating','Chance of Admit ':'Chance'},inplace=True)
print('Mean CGPA Score is :',int(df[df['CGPA']<=500].CGPA.mean()))
print('Mean GRE Score is :',int(df[df['GRE']<=500].GRE.mean()))
print('Mean TOEFL Score is :',int(df[df['TOEFL']<=500].TOEFL.mean()))
print('Mean University rating is :',int(df[df['UnivRating']<=500].UnivRating.mean()))

# %% [code]
# Scores for a 90%+ chance
df_sort=df.sort_values(by=df.columns[-1],ascending=False)
df_sort.head()
df_sort[(df_sort['Chance']>0.90)].mean().reset_index()

# %% [code]
# Now we dont need the columns Srno, Gender and Nation so we drop them
# Because these factors don't affect chance of getting admission
df.drop(['Srno', 'Gender', 'Nation'], axis=1, inplace=True)
df.head()

# %% [code]
# Split the dataset into training and testing set and prepare the inputs and outputs
from sklearn.model_selection import train_test_split

X = df.drop(['Chance'], axis=1)
y = df['Chance']
X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.20, shuffle=False)

# %% [code]
# Use a bunch of different algorithms to see which model performs better
from sklearn.metrics import accuracy_score
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.svm import SVR
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
from sklearn.linear_model import Lasso,Ridge,BayesianRidge,ElasticNet,HuberRegressor,LinearRegression,LogisticRegression,SGDRegressor
from sklearn.metrics import mean_squared_error

models = [['DecisionTree :',DecisionTreeRegressor()],
           ['Linear Regression :', LinearRegression()],
           ['RandomForest :',RandomForestRegressor()],
           ['KNeighbours :', KNeighborsRegressor(n_neighbors = 2)],
           ['SVM :', SVR()],
           ['AdaBoostClassifier :', AdaBoostRegressor()],
           ['GradientBoostingClassifier: ', GradientBoostingRegressor()],
           ['Xgboost: ', XGBRegressor()],
           ['CatBoost: ', CatBoostRegressor(logging_level='Silent')],
           ['Lasso: ', Lasso()],
           ['Ridge: ', Ridge()],
           ['BayesianRidge: ', BayesianRidge()],
           ['ElasticNet: ', ElasticNet()],
           ['HuberRegressor: ', HuberRegressor()]]

print("Results...")

# Calculate and display the Mean Squared Error for each algorithm 
for name,model in models:
    model = model
    model.fit(X_train, y_train)
    predictions = model.predict(X_test)
    print(name, (np.sqrt(mean_squared_error(y_test, predictions))))

# %% [code]
# Determine the features that are most and least important for getting admission
classifier = RandomForestRegressor()
classifier.fit(X,y)
feature_names = X.columns
importance_frame = pd.DataFrame()
importance_frame['Features'] = X.columns
importance_frame['Importance'] = classifier.feature_importances_
importance_frame = importance_frame.sort_values(by=['Importance'], ascending=True)

plt.barh([1,2,3,4,5,6,7], importance_frame['Importance'], align='center', alpha=0.5, color = 'darkgreen')
plt.yticks([1,2,3,4,5,6,7], importance_frame['Features'])
plt.xlabel('Importance')
plt.title('Feature Importances')
plt.show()

# %% [code]
# Finally, predict a student's chance based on his scores
# Use Linear Regression as it has lowest error/ better accuracy

reg=LinearRegression()
reg.fit(X_train,y_train)

# Uncomment the following lines of code to take input from a student/user

# print("Enter your scores for GRE, TOEFL, University Rating, SOP, LOR, CGPA, Research:")
# print("Indicate Research as 0 and 1 for 'No' and 'Yes' respectively")
# Score = []
# for i in range(7): 
#     ele = input()
#     Score.append(ele) 
    
    
Score=['337','118','4','4.5','4.5','9.65','0'] # hardcoded scores
Score=pd.DataFrame(Score).T
chance=reg.predict(Score)
print(chance[0]*100)
